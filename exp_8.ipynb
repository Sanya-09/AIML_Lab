# importing necessry library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA 
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# 1. load iris datasets
# the iris dataset contain 150 sample , with 4 features and 3 classes of flowers
iris = load_iris()
x = iris.data          # features : sepal length , sepal width, petal length , petal width.
y = iris.target        # target : species (Sentosa , Versicolor , Virginica)
# convert to dataframe for better visulization and handling

df = pd.DataFrame(data = x , columns=iris.feature_names)
df['species'] = iris.target_names[y]
print("First few row of dataset:")
print(df.head())
#2. standarize the data
# PCA  is sensitive to scale of the features, so we need to standarize the data.
# standardScaler will normalize the features to have mean 0 and standard deviation 1. 

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)
# 3 Apply PCA for dimensionally reduction
# we reduce the dimensionally to 2 components for easy visualization 

pca = PCA(n_components = 2)
x_pca = pca.fit_transform(x_scaled)
# create a new dataframe conataining the two principal components and the species label
df_pca = pd.DataFrame(data = x_pca , columns = ['PC1','PC2'])
df_pca['species'] = iris.target_names[y]

print("First few rows after PCA transformation:")
print(df_pca.head())
# 4 explained variance ration
# this tells us how much variance each principalcomponent from the original data .

explained_variance_ratio = pca.explained_variance_ratio_
print("Explained variation by each principal component: (explained_variance_ration)")
print("Total variation explained by two component: [sum(explained_variance_ration)]")
# 5. visulaize the pca result
# we will plot two principal components (pc1 and pc2) to see how well pca separated the classes
plt.figure(figsize= (0 , 6))
colors = ['r','g','b']
species = ['setosa','versicolor','virginica']

# print each classes with different colors
for i, species_name in enumerate(species):
    subset = df_pca[df_pca['species']== species_name]
    plt.scatter(subset['PC1'],subset['PC2'], label=species_name,c = colors[i],s=50)
# add labels and legend
plt.xlabel('Principal Component 1:')
plt.ylabel('Principal Component 2:')
plt.title('2D PCA of the Iris dataset')
plt.legend()
plt.grid(True)
plt.show()
# 6 cumulative explained variance
# cumulative explained variance tells us how much of the original variance we reatain as we keep adding more components 
pca_full = PCA(n_components = x.shape[1])  # keep all components to see the cumulative effect
pca_full.fit(x_scaled)

cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
print("\n Cumulative explained variance for all principal components:")
print(cumulative_variance)
# plot the cumulative explained variance
plt.figure(figsize = (8,6))
plt.plot(range(1, x.shape[1] + 1) , cumulative_variance , marker= 'o', linestyle= '--' , color = 'b')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance ')
plt.title('Cumulative explained variance as a function of the number of components')
plt.grid(True)
plt.show()
